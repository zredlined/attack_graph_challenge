GRPO Attack Graph Challenge - AI Agent Cybersecurity Reasoning TrainingTraining autonomous AI agents to perform cybersecurity penetration testing using Group Relative Policy Optimization (GRPO). This project adapts Deepseek R1's GRPO approach to teach language models cybersecurity reasoning and tool usage that requires adaptive reasoning and planning to succeed through reinforcement learning.OverviewThis repository implements GRPO training to teach language models to navigate network attack graphs autonomously, demonstrating emergent cybersecurity reasoning behavior without human feedback. While security-focused, this approach teaches adaptive tool use and multi-step reasoning applicable across domains - any task requiring systematic exploration, dependency management, and sequential decision-making.The agent learns to:Explore networks under "fog of war" conditionsExecute multi-step attack sequences with tool dependenciesFilter signal from noise by ignoring decoy vulnerabilities ("red herrings")Achieve Domain Admin access through systematic penetration testingHow Success is MeasuredAn agent's generated attack plan is evaluated by a stateful simulator. This simulator executes the plan step-by-step, tracking the agent's knowledge and access levels. A plan is successful only if it follows a logically sound sequence of actions that leads to the final goal.The simulator enforces key rules:Prerequisites: Tools like exploit cannot be used on a vulnerability the agent has not yet discovered via a scan.Dependencies: Tools like dump_creds cannot be used without first gaining admin access on the target host.Accuracy: The agent must use the specific information (e.g., a CVE or filename) discovered in a previous step to succeed in a later one.The Core Challenge: Learning to ReasonThe agent is trained to generate a complete, multi-step plan in a single pass. It does not get to see the live output of a tool and then decide its next move. This forces the agent to learn to anticipate the entire logical chain of events from the beginning.The Placeholder SystemTo achieve this, the agent learns to use a placeholder syntax to link steps together. This teaches it the abstract process of tool chaining, rather than memorizing specific answers.Example of a Successful Plan:<think>
I will scan the WebServer first. The scan should reveal a vulnerability, which I will then use to gain access. After that, I will dump credentials to pivot to the next server.
</think>
<answer>
1. scan(WebServer)
2. exploit(WebServer, @step1_cve)
3. dump_creds(WebServer)
4. use_creds(DBServer, @step3_creds)
</answer>
The "Aha!" Moment: Ignoring Red HerringsIn more complex scenarios, the scan tool will reveal both a correct path (e.g., a readable config file) and a decoy (a fake, non-functional CVE). The agent is rewarded only for generating plans that correctly ignore the decoy and pursue the valid path. This is the core test of its reasoning ability.The Experiment: Mastery and AdaptationThis project is structured as a two-phase experiment to demonstrate not only that the agent can learn a task, but that it can adapt when the rules of its environment change. This simulates combating "agent drift" in a real-world, dynamic threat landscape.Phase 1: MasteryFirst, the agent is trained on a biased dataset where the optimal strategy is overwhelmingly to ignore decoy CVEs and read configuration files (the misconfig_path). The goal is to see if the agent can master this specific reality.Expected Outcome: The agent's win rate will rapidly climb to near 100% as it learns to favor the read_file tool and ignore the tempting but incorrect exploit decoys.Phase 2: Adaptation (The Reversal Experiment)Next, the fully trained model from Phase 1 is put into a new, reversed reality. We continue its training on a new dataset where the bias is flipped: the optimal strategy is now to ignore decoy files and use the exploit.Expected Outcome:The Shock: The agent's win rate will initially plummet as its old, trusted "file-first" strategy now leads to failure.The Adaptation: Through continued reinforcement learning, the agent will unlearn its old bias and adapt to the new reality. Its win rate will climb back to near 100% as it learns to favor the exploit tool.This reversal proves the agent has learned a generalizable reasoning process, not just a simple, brittle rule.Training Progress & ResultsThis table reflects the actual, accelerated learning curve observed during Phase 1 (Mastery), using the biased dataset and the stricter simulator.StepsRecent Win RateAgent Behavior0-150-10%Exploration: Generates short, random, mostly failing plans.15-5010-60%"Aha!" Moment: Discovers that long, chained plans yield high rewards. Win rate rapidly increases.50-10060-90%Refinement: Masters the optimal strategy for the biased dataset, consistently generating correct plans.100-16090-100%Mastery: Reaches peak performance, solving the puzzle with near-perfect accuracy in every attempt.Generating a Biased Dataset for the ExperimentTo replicate the "Mastery and Adaptation" experiment, you need to generate datasets with a skewed path distribution.# Generate the Phase 1 "file-biased" dataset (80% misconfig path)
python scripts/dataset_generation/generate_attack_dataset.py \
  --difficulty complex \
  --path_bias 0.1 0.8 0.1 \
  --num_samples 50000 \
  --output_file file_biased_dataset.jsonl

# Generate the Phase 2 "exploit-biased" dataset for the reversal
python scripts/dataset_generation/generate_attack_dataset.py \
  --difficulty complex \
  --path_bias 0.8 0.1 0.1 \
  --num_samples 50000 \
  --output_file exploit_biased_dataset.jsonl